import streamlit as st
import joblib
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime


# Try to import the Hugging Face transformers library.  If it's not
# available (for instance due to missing dependencies), we set a
# flag so that the app can fall back to deterministic responses.
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore
    _transformers_available = True
except ImportError:
    _transformers_available = False


# -----------------------------------------------------------------------------
# Model and vectorizer loading
#
# The following section loads the vocabulary and inverse document frequency
# values used to initialize the TFâ€‘IDF vectorizer.  It then loads a
# preâ€‘trained XGBoost model from disk.  These files must reside in the
# same directory as this script.  If they are missing, Streamlit will
# display an appropriate error when the app is executed.
# -----------------------------------------------------------------------------

with open("vocab.json", "r", encoding="utf-8") as f:
    vocab = json.load(f)

# Load IDF values and attach them to a fresh vectorizer
idf = np.load("idf.npy")
tfidf_vectorizer = TfidfVectorizer(vocabulary=vocab)
tfidf_vectorizer.idf_ = idf

# Load the classification model
xgb_model = joblib.load("xgbc_nlp_depression_level_model.pkl")

label_map = {0: "ì •ìƒ", 1: "ê²½ë¯¸í•œ ìš°ìš¸ì¦", 2: "ì¤‘ë“±ë„ ìš°ìš¸ì¦"}


# -----------------------------------------------------------------------------
# Explanation generation
#
# When a prediction is made, this helper function attempts to use a
# lightweight language model to generate a helpful explanation.  If
# transformers are unavailable or an error occurs during generation,
# predefined fallback text is returned based on the predicted label.
# -----------------------------------------------------------------------------

def generate_llm_explanation(
    user_text: str,
    pred_label: str,
    llm_model_name: str = "distilgpt2",
    device: str = "cpu",
) -> str:
    """
    Generate a natural language explanation for the predicted label.

    This function attempts to explain the model's prediction by
    summarising the user's statement and inferring potential causes of
    their depressive feelings.  When a language model is available, it
    generates a short empathetic explanation that interprets possible
    stressors or triggers mentioned in the input.  If the model
    cannot be loaded, fallback text is used.

    Parameters
    ----------
    user_text : str
        The original user input text describing their feelings.
    pred_label : str
        The predicted category name (e.g. "ì •ìƒ", "ê²½ë¯¸í•œ ìš°ìš¸ì¦").
    llm_model_name : str, optional
        Name of the Hugging Face model to use for generation.  Default
        is ``"distilgpt2"`` because of its small size and permissive
        license.
    device : str, optional
        Device for model execution (e.g. ``"cpu"`` or ``"cuda"``).  Default
        is CPU.

    Returns
    -------
    str
        A Korean explanation describing the prediction in 3â€“5 sentences,
        highlighting possible causes mentioned in the user input.
    """
    # Prompt instructing the language model to infer causes from the input
    prompt = (
        f"ì‚¬ìš©ìì˜ ì§„ìˆ : {user_text}\n"
        f"ëª¨ë¸ì˜ ì˜ˆì¸¡ ìš°ìš¸ì¦ ì¤‘ì¦ë„: {pred_label}\n"
        "ì‚¬ìš©ìê°€ ì„œìˆ í•œ ë‚´ìš©ì—ì„œ ìš°ìš¸ê°ì„ ìœ ë°œí•œ ì£¼ìš” ì›ì¸ì´ë‚˜ ìŠ¤íŠ¸ë ˆìŠ¤ë¡œ ì¶”ì¸¡ë˜ëŠ” ë¶€ë¶„ì„ ì°¾ì•„ "
        "í•´ì„í•´ ì£¼ì„¸ìš”. ê²°ê³¼ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ 3~5ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ê³µê°ì–´ë¦° ì–´ì¡°ë¡œ ê°ì • ê´€ë¦¬ì™€ "
        "ìŠ¤íŠ¸ë ˆìŠ¤ ì™„í™” íŒì„ í¬í•¨í•´ ì£¼ì„¸ìš”. ì˜ë£Œì  ì§„ë‹¨ì€ í•˜ì§€ ë§ˆì„¸ìš”."
    )
    # Use a language model if available
    if _transformers_available:
        try:
            tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
            model = AutoModelForCausalLM.from_pretrained(llm_model_name)
            gen = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=0 if device == "cuda" else -1,
            )
            result = gen(
                prompt,
                max_length=len(prompt.split()) + 80,
                num_return_sequences=1,
                do_sample=True,
                temperature=0.8,
            )
            generated = result[0]["generated_text"]
            # Remove the prompt portion and return the explanation
            # If the model echoes the prompt, split on the last newline
            explanation_lines = generated.split("\n")
            return explanation_lines[-1].strip()
        except Exception:
            pass
    # Fallback explanations based solely on the predicted label
    if pred_label == "ì •ìƒ":
        return (
            "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì •ìƒ ë²”ì£¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. ìš°ìš¸ê°ì„ ëŠë¼ì§€ ì•Šë”ë¼ë„ ê·œì¹™ì ì¸ ìš´ë™ê³¼ ì¶©ë¶„í•œ ìˆ˜ë©´, "
            "ê· í˜• ì¡íŒ ì‹ë‹¨, ê°ì‚¬ì¼ê¸° ì“°ê¸° ë“± ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ê¾¸ì¤€íˆ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì •ì‹ ê±´ê°•ì— ë„ì›€ì´ ë©ë‹ˆë‹¤."
        )
    if pred_label == "ê²½ë¯¸í•œ ìš°ìš¸ì¦":
        return (
            "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ëŠë¼ëŠ” ìƒí™©ì„ ì ê²€í•˜ê³ , "
            "ìš´ë™Â·ëª…ìƒÂ·ê°ì‚¬ì¼ê¸° ë“±ìœ¼ë¡œ ë§ˆìŒì„ ë‹¤ìŠ¤ë ¤ ë³´ì„¸ìš”. ê±±ì •ì´ ì§€ì†ë˜ë©´ ì£¼ë³€ì˜ ì§€ì§€ë¥¼ ë°›ê±°ë‚˜ ì „ë¬¸ê°€ ìƒë‹´ì„ í†µí•´ ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    # pred_label == "ì¤‘ë“±ë„ ìš°ìš¸ì¦" or unknown
    return (
        "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ë‚˜ ë‹¤ì–‘í•œ ë¬¸ì œë“¤ì´ ì˜í–¥ì„ ë¯¸ì³¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
        "ì¶©ë¶„í•œ íœ´ì‹ê³¼ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì„ ìœ ì§€í•˜ê³ , ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ì´ì•¼ê¸°í•˜ê±°ë‚˜ ì „ë¬¸ê°€ì—ê²Œ ë„ì›€ì„ ìš”ì²­í•˜ì„¸ìš”. ì‹¬ê°í•œ ì¦ìƒì´ ì§€ì†ë˜ë©´ ì¹˜ë£Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
    )


# -----------------------------------------------------------------------------
# Chatbot helper
#
# This function creates a reply to the user's followâ€‘up questions.  It uses
# a lightweight language model when available and otherwise returns
# preâ€‘defined advice.  The chatbot emphasises that it provides general
# guidance without medical diagnosis.  If a prediction was previously
# generated, the last predicted label can be supplied to contextualise the
# answer.
# -----------------------------------------------------------------------------

def chatbot_answer(user_msg: str, last_pred_label: str | None = None,
                   llm_model_name: str = "distilgpt2") -> str:
    """
    Generate a chatbot reply for a user's question.

    Parameters
    ----------
    user_msg : str
        The user's followâ€‘up question or comment.
    last_pred_label : str or None
        The most recent predicted label from the classification model (if any).
    llm_model_name : str, optional
        Name of the lightweight language model to use.  Default is "distilgpt2".

    Returns
    -------
    str
        A friendly, empathetic reply in Korean.
    """
    # Base prompt describing the assistant's persona and safety constraints
    base_prompt = (
        "ë„ˆëŠ” ìš°ìš¸ì¦ ê´€ë ¨ ìƒë‹´ AIì…ë‹ˆë‹¤. ì§„ë‹¨ì„ ë‚´ë¦¬ì§€ ì•Šê³ , ì‚¬ìš©ìê°€ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì •ë³´ì™€ "
        "ê°ì • ì¡°ì ˆ íŒ, ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ ë°©ë²•, ìƒí™œìŠµê´€ ê°œì„  ì¡°ì–¸ ë“±ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ ìš°ìš¸ì¦ ì˜ˆì¸¡ ê²°ê³¼ì˜ "
        "ì˜ë¯¸ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë•ì§€ë§Œ ì „ë¬¸ì ì¸ ì˜ë£Œ íŒë‹¨ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
    )
    if last_pred_label:
        base_prompt += f"ì°¸ê³ ë¡œ ìµœê·¼ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ëŠ” '{last_pred_label}' ì…ë‹ˆë‹¤.\n"
    base_prompt += (
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê³µê°ê°€ëŠ” ì–´ì¡°ë¡œ 3~5ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    )
    prompt = base_prompt + f"\nì‚¬ìš©ì: {user_msg}\nAI:"  # Format conversation
    # Use transformers if available
    if _transformers_available:
        try:
            tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
            model = AutoModelForCausalLM.from_pretrained(llm_model_name)
            gen = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=-1,
            )
            response = gen(
                prompt,
                max_length=len(prompt.split()) + 60,
                num_return_sequences=1,
                do_sample=True,
                temperature=0.8,
            )
            text = response[0]["generated_text"]
            # Extract the assistant's part after the last "AI:" marker
            return text.split("AI:")[-1].strip()
        except Exception:
            pass
    # Determine severity-specific suggestion templates
    # Each severity has specific advice for different query categories.  If no
    # prediction is available, a general template is used.
    severity_templates = {
        "ì •ìƒ": {
            "stress": "í˜„ì¬ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì •ìƒ ë²”ì£¼ì…ë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ì‚¬, ëª…ìƒê³¼ ê°ì‚¬ì¼ê¸°ë¥¼ í™œìš©í•´ ì¢‹ì€ ìƒíƒœë¥¼ ìœ ì§€í•˜ì„¸ìš”.",
            "emotion": "í˜„ì¬ ì •ìƒ ë²”ì£¼ì´ì§€ë§Œ ê°ì • ê´€ë¦¬ë¥¼ ìœ„í•´ ë§ˆìŒì±™ê¹€ê³¼ í˜¸í¡ë²•, ê°ì • ì¼ê¸°, ê¸ì •ê³¼ ë¶€ì • ê°ì •ì˜ ê· í˜•ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.",
            "lifestyle": "í˜„ì¬ ì •ìƒ ë²”ì£¼ì…ë‹ˆë‹¤. ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ê¾¸ì¤€í•œ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ì¹´í˜ì¸Â·ì•Œì½”ì˜¬ ì œí•œ, ì·¨ë¯¸ í™œë™ì„ ì´ì–´ê°€ì„¸ìš”.",
            "result": "í˜„ì¬ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì •ìƒ ë²”ì£¼ë¡œ íŠ¹ë³„í•œ ìš°ë ¤ëŠ” ì—†ìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ëŠë‚„ ë•Œì—ëŠ” ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ ê¸°ë²•ì„ í™œìš©í•˜ë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤.",
            "default": "í˜„ì¬ ì •ìƒ ë²”ì£¼ì— ì†í•´ ìˆìœ¼ë‹ˆ ì‹ ì²´ì™€ ë§ˆìŒì˜ ê±´ê°•ì„ ì§€í‚¤ê¸° ìœ„í•´ ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ë²•ì„ ê¾¸ì¤€íˆ ì‹¤ì²œí•˜ì„¸ìš”. ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
        },
        "ê²½ë¯¸í•œ ìš°ìš¸ì¦": {
            "stress": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨ê³¼ í•¨ê»˜ ëª…ìƒì´ë‚˜ ê°ì‚¬ì¼ê¸°ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”. ì£¼ë³€ ì‚¬ëŒë“¤ê³¼ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒë„ ë„ì›€ì´ ë©ë‹ˆë‹¤.",
            "emotion": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë‹ˆ ë§ˆìŒì±™ê¹€, í˜¸í¡ë²•, ê°ì • ì¼ê¸° ë“±ì„ í™œìš©í•´ ê°ì •ì„ ê´€ë¦¬í•´ë³´ì„¸ìš”. ê¸ì •ì ì¸ ê²½í—˜ì— ì§‘ì¤‘í•˜ê³ , ê±±ì •ì„ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ì†Œí†µí•˜ì„¸ìš”.",
            "lifestyle": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë¯€ë¡œ ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ë”ìš± ì‹ ê²½ ì¨ì•¼ í•©ë‹ˆë‹¤. ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ì¹´í˜ì¸Â·ì•Œì½”ì˜¬ ì ˆì œ, í¡ì—°ê³¼ ì•½ë¬¼ í”¼í•˜ê¸° ë“±ì´ ë„ì›€ì´ ë©ë‹ˆë‹¤. ë˜í•œ ì¦ê±°ì›€ì„ ëŠë‚„ ìˆ˜ ìˆëŠ” ì·¨ë¯¸ì™€ í™œë™ì„ ì§€ì†í•˜ì„¸ìš”.",
            "result": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆì§€ë§Œ ì ì ˆí•œ ê´€ë¦¬ë¡œ ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê±±ì •ì´ ì§€ì†ë˜ë©´ ìƒë‹´ì„ ê¶Œìœ ë°›ìœ¼ì‹œê³ , ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ì™€ ìƒí™œìŠµê´€ ê°œì„ ì„ í†µí•´ ì™„í™”ë¥¼ ë„ëª¨í•˜ì„¸ìš”.",
            "default": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë‹ˆ ê¸°ë¶„ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ë²•ì„ ê¾¸ì¤€íˆ ì‹¤ì²œí•˜ê³ , í•„ìš”í•˜ë©´ ì£¼ë³€ ì‚¬ëŒë“¤ê³¼ ì´ì•¼ê¸°í•˜ê±°ë‚˜ ìƒë‹´ì„ ê³ ë ¤í•´ë³´ì„¸ìš”. ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”."
        },
        "ì¤‘ë“±ë„ ìš°ìš¸ì¦": {
            "stress": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ê°€ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤. ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ëª…ìƒê³¼ ê°ì‚¬ì¼ê¸°ë¥¼ ì‹¤ì²œí•˜ì„¸ìš”. ë˜í•œ, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ë§ˆìŒì„ ë‚˜ëˆ„ê³  í•„ìš”í•˜ë‹¤ë©´ ì „ë¬¸ì ì¸ ìƒë‹´ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.",
            "emotion": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë¯€ë¡œ ê°ì • ê´€ë¦¬ì— ë”ìš± ì‹ ê²½ì„ ì¨ì•¼ í•©ë‹ˆë‹¤. ë§ˆìŒì±™ê¹€ê³¼ í˜¸í¡ë²•ì„ í†µí•´ ê°ì •ì„ ì •ë¦¬í•˜ê³ , ê°ì •ì¼ê¸°ë¥¼ ì¨ë³´ì„¸ìš”. ì£¼ë³€ì˜ ì§€ì§€ë§ì„ í™œìš©í•˜ê³  ë¶€ì •ì ì¸ ìƒê°ì„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ê³µìœ í•˜ì„¸ìš”.",
            "lifestyle": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë¯€ë¡œ ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ë”ë¶ˆì–´ ì „ë¬¸ì ì¸ ì§€ì›ì„ ë°›ì„ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ê¾¸ì¤€í•œ ìš´ë™ê³¼ ê· í˜• ì¡íŒ ì‹ë‹¨, ì¶©ë¶„í•œ ìˆ˜ë©´, ì¹´í˜ì¸Â·ì•Œì½”ì˜¬ ì œí•œ, í¡ì—°Â·ì•½ë¬¼ í”¼í•˜ê¸°ë¥¼ ì‹¤ì²œí•˜ì„¸ìš”. ì‹¬ê°í•œ ì¦ìƒì´ ì§€ì†ë  ê²½ìš° ì •ì‹ ê±´ê°• ì „ë¬¸ê°€ì—ê²Œ ìƒë‹´ì„ ë°›ì•„ë³´ì„¸ìš”.",
            "result": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬, ì „ë¬¸ì ì¸ ë„ì›€ì„ ë°›ê³  ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ë²•ì„ ì‹¤ì²œí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì£¼ë³€ì¸ê³¼ì˜ ì†Œí†µê³¼ ìƒë‹´ì„ í†µí•´ ì§€ì§€ë¥¼ ë°›ìœ¼ì„¸ìš”.",
            "default": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë¯€ë¡œ ìŠ¤ìŠ¤ë¡œë¥¼ ëŒë³´ê³  ì „ë¬¸ì ì¸ ì§€ì›ì„ ë°›ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ì™€ ìƒí™œìŠµê´€ ê°œì„ ì„ ê¾¸ì¤€íˆ ì‹¤ì²œí•˜ê³ , í•„ìš”í•˜ë©´ ì „ë¬¸ê¸°ê´€ì´ë‚˜ ì „ë¬¸ê°€ì˜ ë„ì›€ì„ ë°›ì•„ë³´ì„¸ìš”. ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”."
        }
    }
    # Normalise message for keyword detection
    msg = user_msg.lower()
    # Determine which template to use based on last prediction
    template = severity_templates.get(last_pred_label) if last_pred_label in severity_templates else None
    # Define category keys
    category = None
    if any(keyword in msg for keyword in ["ìŠ¤íŠ¸ë ˆìŠ¤", "stress"]):
        category = "stress"
    elif any(keyword in msg for keyword in ["ê°ì •", "ì¡°ì ˆ", "emotion"]):
        category = "emotion"
    elif any(keyword in msg for keyword in ["ìƒí™œ", "ìŠµê´€", "lifestyle"]):
        category = "lifestyle"
    elif any(keyword in msg for keyword in ["ê²°ê³¼", "ì˜ˆì¸¡", "í•´ì„"]):
        category = "result"
    # If we have a template for this severity, return the corresponding message
    if template:
        if category and category in template:
            return template[category]
        # default category message
        return template["default"]
    # Generic responses when no prediction is available
    generic = {
        "stress": "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ì‹¬í˜¸í¡ê³¼ ëª…ìƒ, ê°ì‚¬ì¼ê¸°, ìì—° ì† ì‚°ì±…, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ì˜ ëŒ€í™”ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.",
        "emotion": "ê°ì •ì„ ì¡°ì ˆí•˜ê¸° ìœ„í•´ ë§ˆìŒì±™ê¹€ê³¼ í˜¸í¡ë²•ì„ í†µí•´ í˜„ì¬ ìˆœê°„ì— ì§‘ì¤‘í•˜ê³ , ê°ì •ì¼ê¸°ë¥¼ ì¨ì„œ ê°ì •ì„ í‘œí˜„í•´ë³´ì„¸ìš”. ê¸ì •ê³¼ ë¶€ì • ê°ì •ì˜ ê· í˜•ì„ ìœ ì§€í•˜ê³  ê°ì‚¬í•˜ëŠ” ë§ˆìŒì„ ê°–ëŠ” ê²ƒì´ ë„ì›€ì´ ë©ë‹ˆë‹¤.",
        "lifestyle": "ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ìœ„í•´ ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ì¹´í˜ì¸Â·ì•Œì½”ì˜¬ ì œí•œ, í¡ì—°Â·ì•½ë¬¼ í”¼í•˜ê¸°ë¥¼ ì‹¤ì²œí•˜ì„¸ìš”. ê°€ì¡±ê³¼ ì¹œêµ¬ë“¤ê³¼ ì‹œê°„ì„ ë³´ë‚´ê³  ì·¨ë¯¸ í™œë™ì„ ì´ì–´ê°€ëŠ” ê²ƒë„ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "result": "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤. ê²°ê³¼ê°€ ê±±ì •ë˜ë©´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ìƒë‹´í•˜ê±°ë‚˜ ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì„ êµ¬í•˜ì„¸ìš”. ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ê°€ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "default": "ì§ˆë¬¸ì„ í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì €ëŠ” ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ì œê³µí•˜ëŠ” ìƒë‹´ AIì´ë©°, ì§„ë‹¨ì„ ë‚´ë¦¬ê±°ë‚˜ ì¹˜ë£Œë¥¼ ëŒ€ì‹ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒí™œìŠµê´€ ê°œì„ ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬, ê¸ì •ì ì¸ ë§ˆìŒê°€ì§ì„ í†µí•´ ì •ì‹ ê±´ê°•ì„ ëŒë³´ì„¸ìš”. ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
    }
    if category and category in generic:
        return generic[category]
    return generic["default"]


# -----------------------------------------------------------------------------
# Streamlit application layout
#
# The following section defines the user interface elements: text input for
# describing current feelings, a prediction trigger button, display of
# probabilities and interpretive information, and a chat interface for
# followâ€‘up questions.
# -----------------------------------------------------------------------------

st.set_page_config(page_title="ìš°ìš¸ì¦ ì˜ˆì¸¡ ë° ìƒë‹´", layout="centered")
st.title("ğŸ§  ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ìš°ìš¸ì¦ ì˜ˆì¸¡ ë° ìƒë‹´")

st.markdown(
    "ë³¸ ì†Œí”„íŠ¸ì›¨ì–´ëŠ” ë¶„ë‹¹ì°¨ë³‘ì› ì •ì‹ ê±´ê°•ì˜í•™ê³¼ ì „ë¬¸ì˜ì˜ ìš°ìš¸ì¦ ì§„ë‹¨ ê²½í—˜ì„ "
    "ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ í•™ìŠµí•˜ì—¬, ë¬¸ì¥ë§Œìœ¼ë¡œ ìš°ìš¸ì¦ ì¤‘ì¦ë„ë¥¼ ì˜ˆì¸¡í•˜ê³  ê´€ë ¨ ìƒë‹´ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. "
    "**ì´ ì•±ì˜ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©° ì •ì‹ ê±´ê°•ì˜í•™ê³¼ ì „ë¬¸ì˜ì˜ ì§„ë‹¨ì„ ëŒ€ì²´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**"
)

# User input area
user_input = st.text_area("í˜„ì¬ ëŠë¼ê³  ìˆëŠ” ê°ì •ì„ ë¬¸ì¥ìœ¼ë¡œ ì…ë ¥í•´ë³´ì„¸ìš”.", height=150)

# Initialize session state for predictions and chat
if "last_pred_label" not in st.session_state:
    st.session_state["last_pred_label"] = None
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []  # list of (role, message)


if st.button("ì§„ë‹¨í•˜ê¸°"):
    if not user_input.strip():
        st.warning("âš ï¸ ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # Vectorize and predict
        vec = tfidf_vectorizer.transform([user_input])
        probs = xgb_model.predict_proba(vec)[0]
        probs_percent = np.round(probs * 100, 1)
        pred_idx = int(np.argmax(probs_percent))
        pred_label = label_map[pred_idx]
        pred_conf = probs_percent[pred_idx]
        st.session_state["last_pred_label"] = pred_label
        today = datetime.today().strftime("%Y-%m-%d")
        # Display results card
        st.markdown(
            f"""
            <div style="border:2px solid #4CAF50; border-radius:10px; padding:20px; background-color:#f9fff9;">
                <h2 style="color:#2E7D32; text-align:center;">ğŸ§¾ ì¸ê³µì§€ëŠ¥ ì˜ˆì¸¡ ê²°ê³¼</h2>
                <p><b>ë°œê¸‰ì¼ì</b>: {today}</p>
                <p><b>í™˜ì ì§„ìˆ </b>: {user_input}</p>
                <hr>
                <h3 style="color:#1565C0;">ìµœì¢… ì˜ˆì¸¡</h3>
                <p style="font-size:22px; font-weight:bold; color:#D32F2F;">
                    {pred_label} ({pred_conf:.1f}%)
                </p>
                <hr>
                <h3 style="color:#1565C0;">ì „ì²´ ì˜ˆì¸¡ í™•ë¥ </h3>
                <ul>
                    <li>{label_map[0]}: <b>{probs_percent[0]:.1f}%</b></li>
                    <li>{label_map[1]}: <b>{probs_percent[1]:.1f}%</b></li>
                    <li>{label_map[2]}: <b>{probs_percent[2]:.1f}%</b></li>
                </ul>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.success("âœ… ì˜ˆì¸¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¬ê°í•œ ì¦ìƒì´ ì§€ì†ë  ê²½ìš° ì „ë¬¸ì˜ ìƒë‹´ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        # Display explanation
        explanation = generate_llm_explanation(user_input, pred_label)
        st.markdown("### AI ì„¤ëª…")
        st.write(explanation)

# Separator
st.markdown("---")
st.subheader("ğŸ’¬ AI ìƒë‹´ ì±—ë´‡")

# Display previous chat messages
for role, message in st.session_state["chat_history"]:
    with st.chat_message(role):
        st.markdown(message)

# Chat input using a text field and send button.  `st.chat_input` is
# only available in recent Streamlit versions; using `st.text_input` ensures
# compatibility on older runtimes.
col1, col2 = st.columns([4, 1])
with col1:
    chat_prompt = st.text_input(
        "ì±—ë´‡ì—ê²Œ ì§ˆë¬¸í•˜ê¸° (ìš°ìš¸ì¦ ê´€ë ¨ ì¼ë°˜ ìƒë‹´, ê°ì • ì¡°ì ˆ, ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ ë“±ì„ ì…ë ¥í•˜ì„¸ìš”.)",
        key="chat_input",
    )
with col2:
    send_clicked = st.button("ì „ì†¡", key="send_chat")

# When the user submits a question
if send_clicked and chat_prompt:
    # Record the user question
    st.session_state["chat_history"].append(("user", chat_prompt))
    with st.container():
        with st.chat_message("user"):
            st.markdown(chat_prompt)
    # Generate reply using last predicted label if available
    reply = chatbot_answer(chat_prompt, st.session_state.get("last_pred_label"))
    st.session_state["chat_history"].append(("assistant", reply))
    with st.container():
        with st.chat_message("assistant"):
            st.markdown(reply)


# Footer with additional information
st.markdown("---")
st.markdown("### ì°¸ê³  ì •ë³´")
st.markdown("* Patent title: APPARATUS AND METHOD FOR PREDICTING DEPRESSION LEVELS USING NATURAL LANGUAGE PROCESSING AND EXPLAINABLE ARTIFICIAL INTELLIGENCE")
st.markdown("* Patent number: 10-2024-0119065")
st.markdown("* Developer: Myung-Gwan Kim")
st.markdown("* Applicant: CHA University Industry-Academic Cooperation Foundation")
st.markdown("* Inventors: Myung-Gwan Kim, Hyun Wook Han, DaWoon Wang, JoonHo Park")
