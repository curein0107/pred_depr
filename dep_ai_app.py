import streamlit as st
import joblib
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime

# Try to import the Hugging Face transformers library.  If it's not
# available (for instance due to missing dependencies), we set a
# flag so that the app can fall back to deterministic responses.
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore
    _transformers_available = True
except ImportError:
    _transformers_available = False


# -----------------------------------------------------------------------------
# Model and vectorizer loading
#
# The following section loads the vocabulary and inverse document frequency
# values used to initialize the TFâ€‘IDF vectorizer.  It then loads a
# preâ€‘trained XGBoost model from disk.  These files must reside in the
# same directory as this script.  If they are missing, Streamlit will
# display an appropriate error when the app is executed.
# -----------------------------------------------------------------------------

with open("vocab.json", "r", encoding="utf-8") as f:
    vocab = json.load(f)

# Load IDF values and attach them to a fresh vectorizer
idf = np.load("idf.npy")
tfidf_vectorizer = TfidfVectorizer(vocabulary=vocab)
tfidf_vectorizer.idf_ = idf

# Load the classification model
xgb_model = joblib.load("xgbc_nlp_depression_level_model.pkl")

label_map = {0: "ì •ìƒ", 1: "ê²½ë¯¸í•œ ìš°ìš¸ì¦", 2: "ì¤‘ë“±ë„ ìš°ìš¸ì¦"}


# -----------------------------------------------------------------------------
# Explanation generation
#
# When a prediction is made, this helper function attempts to use a
# lightweight language model to generate a helpful explanation.  If
# transformers are unavailable or an error occurs during generation,
# predefined fallback text is returned based on the predicted label.
# -----------------------------------------------------------------------------

def generate_llm_explanation(user_text: str, pred_label: str,
                             llm_model_name: str = "distilgpt2",
                             device: str = "cpu") -> str:
    """
    Generate a natural language explanation for the predicted label.

    Parameters
    ----------
    user_text : str
        The original user input text describing their feelings.
    pred_label : str
        The predicted category name (e.g. "ì •ìƒ", "ê²½ë¯¸í•œ ìš°ìš¸ì¦").
    llm_model_name : str, optional
        Name of the Hugging Face model to use for generation.  Default is
        "distilgpt2" because of its small size and permissive license.
    device : str, optional
        Device for model execution (e.g. "cpu" or "cuda").  Default is CPU.

    Returns
    -------
    str
        A Korean explanation describing the prediction in 3â€“5 sentences.
    """
    # Construct a prompt instructing the model to summarise the result in
    # Korean, emphasising empathy and avoiding diagnostic language.
    prompt = (
        f"ì‚¬ìš©ìì˜ ì§„ìˆ : {user_text}\n"
        f"ëª¨ë¸ì˜ ì˜ˆì¸¡ ìš°ìš¸ì¦ ì¤‘ì¦ë„: {pred_label}\n"
        "ìœ„ ë‘ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”. "
        "ì¹œì ˆí•˜ê³  ê³µê°ê°€ëŠ” ì–´ì¡°ë¡œ í•œêµ­ì–´ë¡œ 3~5ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. "
        "ì˜ë£Œì  ì§„ë‹¨ì€ í•˜ì§€ ë§ê³ , ì¼ë°˜ì ì¸ ê°ì • ê´€ë¦¬ ë° ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ íŒì„ í¬í•¨í•˜ì„¸ìš”."
    )
    # Attempt to generate text using a transformers pipeline
    if _transformers_available:
        try:
            tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
            model = AutoModelForCausalLM.from_pretrained(llm_model_name)
            generator = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=0 if device == "cuda" else -1,
            )
            result = generator(
                prompt,
                max_length=len(prompt.split()) + 80,
                num_return_sequences=1,
                do_sample=True,
                temperature=0.8,
            )
            generated = result[0]["generated_text"]
            # Remove the prompt from the generated output
            return generated.split("\n")[-1].strip()
        except Exception:
            # If anything fails, fall back to deterministic responses
            pass
    # Fallback explanations for each label
    if pred_label == "ì •ìƒ":
        return (
            "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì •ìƒ ë²”ì£¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. í˜„ì¬ ì…ë ¥í•˜ì‹  ë‚´ìš©ìœ¼ë¡œ ë³´ì•„ í° ìš°ìš¸ ì¦ìƒì€ ë‚˜íƒ€ë‚˜ì§€ "
            "ì•ŠìŠµë‹ˆë‹¤. ê°ì •ì¼ì§€ë¥¼ ì‘ì„±í•˜ê±°ë‚˜ ê·œì¹™ì ì¸ ìƒí™œì„ í†µí•´ ê¸ì •ì ì¸ ìƒíƒœë¥¼ ìœ ì§€í•´ë³´ì„¸ìš”."
        )
    if pred_label == "ê²½ë¯¸í•œ ìš°ìš¸ì¦":
        return (
            "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœê·¼ ê°ì • ë³€í™”ë¥¼ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ê³ , "
            "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì¤„ì´ëŠ” í™œë™ì„ ì‹œë„í•´ë³´ì„¸ìš”. í•„ìš”í•˜ë©´ ì¹œêµ¬ë‚˜ ê°€ì¡±ê³¼ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê±°ë‚˜ ìƒë‹´ì„ "
            "ê³ ë ¤í•´ë³´ëŠ” ê²ƒë„ ë„ì›€ì´ ë©ë‹ˆë‹¤."
        )
    # pred_label == "ì¤‘ë“±ë„ ìš°ìš¸ì¦" or unknown
    return (
        "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìš°ìš¸ê°ì„ ê°ì†Œì‹œí‚¤ê¸° ìœ„í•´ ì¶©ë¶„í•œ íœ´ì‹ê³¼ ê·œì¹™ì ì¸ "
        "ìƒí™œìŠµê´€ì„ ìœ ì§€í•´ë³´ì„¸ìš”. ì „ë¬¸ì ì¸ ìƒë‹´ì´ë‚˜ ì •ì‹ ê±´ê°•ì˜í•™ê³¼ ì „ë¬¸ì˜ì˜ ë„ì›€ì´ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì‹œê¸° "
        "ë°”ëë‹ˆë‹¤."
    )


# -----------------------------------------------------------------------------
# Chatbot helper
#
# This function creates a reply to the user's followâ€‘up questions.  It uses
# a lightweight language model when available and otherwise returns
# preâ€‘defined advice.  The chatbot emphasises that it provides general
# guidance without medical diagnosis.  If a prediction was previously
# generated, the last predicted label can be supplied to contextualise the
# answer.
# -----------------------------------------------------------------------------

def chatbot_answer(user_msg: str, last_pred_label: str | None = None,
                   llm_model_name: str = "distilgpt2") -> str:
    """
    Generate a chatbot reply for a user's question.

    Parameters
    ----------
    user_msg : str
        The user's followâ€‘up question or comment.
    last_pred_label : str or None
        The most recent predicted label from the classification model (if any).
    llm_model_name : str, optional
        Name of the lightweight language model to use.  Default is "distilgpt2".

    Returns
    -------
    str
        A friendly, empathetic reply in Korean.
    """
    # Base prompt describing the assistant's persona and safety constraints
    base_prompt = (
        "ë„ˆëŠ” ìš°ìš¸ì¦ ê´€ë ¨ ìƒë‹´ AIì…ë‹ˆë‹¤. ì§„ë‹¨ì„ ë‚´ë¦¬ì§€ ì•Šê³ , ì‚¬ìš©ìê°€ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì •ë³´ì™€ "
        "ê°ì • ì¡°ì ˆ íŒ, ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ ë°©ë²•, ìƒí™œìŠµê´€ ê°œì„  ì¡°ì–¸ ë“±ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ ìš°ìš¸ì¦ ì˜ˆì¸¡ ê²°ê³¼ì˜ "
        "ì˜ë¯¸ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë•ì§€ë§Œ ì „ë¬¸ì ì¸ ì˜ë£Œ íŒë‹¨ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
    )
    if last_pred_label:
        base_prompt += f"ì°¸ê³ ë¡œ ìµœê·¼ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ëŠ” '{last_pred_label}' ì…ë‹ˆë‹¤.\n"
    base_prompt += (
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê³µê°ê°€ëŠ” ì–´ì¡°ë¡œ 3~5ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    )
    prompt = base_prompt + f"\nì‚¬ìš©ì: {user_msg}\nAI:"  # Format conversation
    # Use transformers if available
    if _transformers_available:
        try:
            tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
            model = AutoModelForCausalLM.from_pretrained(llm_model_name)
            gen = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=-1,
            )
            response = gen(
                prompt,
                max_length=len(prompt.split()) + 60,
                num_return_sequences=1,
                do_sample=True,
                temperature=0.8,
            )
            text = response[0]["generated_text"]
            # Extract the assistant's part after the last "AI:" marker
            return text.split("AI:")[-1].strip()
        except Exception:
            pass
    # Fallback generic responses based on keywords
    msg = user_msg.lower()
    if any(keyword in msg for keyword in ["ìŠ¤íŠ¸ë ˆìŠ¤", "stress"]):
        return (
            "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê¹Šì€ í˜¸í¡ì´ë‚˜ ëª…ìƒê³¼ ê°™ì€ ì´ì™„ ê¸°ë²•ì„ ì‹œë„í•´ ë³´ì„¸ìš”. "
            "ê°€ë²¼ìš´ ì‚°ì±…ì´ë‚˜ ì·¨ë¯¸ í™œë™ë„ ë„ì›€ì´ ë©ë‹ˆë‹¤."
        )
    if any(keyword in msg for keyword in ["ê°ì •", "ì¡°ì ˆ", "emotion"]):
        return (
            "ê°ì •ì„ ì¡°ì ˆí•˜ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” ë§ˆìŒì±™ê¹€ì´ë‚˜ í˜¸í¡ ìš´ë™ì„ í†µí•´ í˜„ì¬ ìˆœê°„ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒì´ ìˆìŠµë‹ˆë‹¤. "
            "ë˜í•œ ê°ì •ì„ ì–µëˆ„ë¥´ê¸°ë³´ë‹¤ ì¼ê¸° ì“°ê¸° ë“±ìœ¼ë¡œ í‘œí˜„í•´ë³´ëŠ” ê²ƒë„ ì¢‹ìŠµë‹ˆë‹¤."
        )
    if any(keyword in msg for keyword in ["ìƒí™œ", "ìŠµê´€", "lifestyle"]):
        return (
            "ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ìœ„í•´ ê· í˜• ì¡íŒ ì‹ì‚¬ì™€ ê·œì¹™ì ì¸ ìš´ë™ì„ ìœ ì§€í•˜ê³ , ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ì„¸ìš”. "
            "ë˜í•œ ì§€ë‚˜ì¹œ ì¹´í˜ì¸ì´ë‚˜ ì•Œì½”ì˜¬ ì„­ì·¨ë¥¼ í”¼í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."
        )
    if any(keyword in msg for keyword in ["ê²°ê³¼", "ì˜ˆì¸¡", "í•´ì„"]):
        return (
            "ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì´ë©°, ê°ì •ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ëŠ” ì§€í‘œì…ë‹ˆë‹¤. "
            "ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ì˜ì˜ ìƒë‹´ì´ í•„ìš”í•¨ì„ ê¸°ì–µí•˜ì„¸ìš”."
        )
    # Default fallback
    return (
        "ì§ˆë¬¸ì„ í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì €ëŠ” ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ì œê³µí•˜ë©°, ì§„ë‹¨ì„ ë‚´ë¦¬ê±°ë‚˜ ì¹˜ë£Œë¥¼ ëŒ€ì‹ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
        "ê°ì • ì¡°ì ˆì´ë‚˜ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´ ì£¼ì„¸ìš”."
    )


# -----------------------------------------------------------------------------
# Streamlit application layout
#
# The following section defines the user interface elements: text input for
# describing current feelings, a prediction trigger button, display of
# probabilities and interpretive information, and a chat interface for
# followâ€‘up questions.
# -----------------------------------------------------------------------------

st.set_page_config(page_title="ìš°ìš¸ì¦ ì˜ˆì¸¡ ë° ìƒë‹´", layout="centered")
st.title("ğŸ§  ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ìš°ìš¸ì¦ ì˜ˆì¸¡ ë° ìƒë‹´")

st.markdown(
    "ë³¸ ì†Œí”„íŠ¸ì›¨ì–´ëŠ” ë¶„ë‹¹ì°¨ë³‘ì› ì •ì‹ ê±´ê°•ì˜í•™ê³¼ ì „ë¬¸ì˜ì˜ ìš°ìš¸ì¦ ì§„ë‹¨ ê²½í—˜ì„ "
    "ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ í•™ìŠµí•˜ì—¬, ë¬¸ì¥ë§Œìœ¼ë¡œ ìš°ìš¸ì¦ ì¤‘ì¦ë„ë¥¼ ì˜ˆì¸¡í•˜ê³  ê´€ë ¨ ìƒë‹´ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. "
    "**ì´ ì•±ì˜ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©° ì •ì‹ ê±´ê°•ì˜í•™ê³¼ ì „ë¬¸ì˜ì˜ ì§„ë‹¨ì„ ëŒ€ì²´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**"
)

# User input area
user_input = st.text_area("í˜„ì¬ ëŠë¼ê³  ìˆëŠ” ê°ì •ì„ ë¬¸ì¥ìœ¼ë¡œ ì…ë ¥í•´ë³´ì„¸ìš”.", height=150)

# Initialize session state for predictions and chat
if "last_pred_label" not in st.session_state:
    st.session_state["last_pred_label"] = None
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []  # list of (role, message)


if st.button("ì§„ë‹¨í•˜ê¸°"):
    if not user_input.strip():
        st.warning("âš ï¸ ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # Vectorize and predict
        vec = tfidf_vectorizer.transform([user_input])
        probs = xgb_model.predict_proba(vec)[0]
        probs_percent = np.round(probs * 100, 1)
        pred_idx = int(np.argmax(probs_percent))
        pred_label = label_map[pred_idx]
        pred_conf = probs_percent[pred_idx]
        st.session_state["last_pred_label"] = pred_label
        today = datetime.today().strftime("%Y-%m-%d")
        # Display results card
        st.markdown(
            f"""
            <div style="border:2px solid #4CAF50; border-radius:10px; padding:20px; background-color:#f9fff9;">
                <h2 style="color:#2E7D32; text-align:center;">ğŸ§¾ ì¸ê³µì§€ëŠ¥ ì˜ˆì¸¡ ê²°ê³¼</h2>
                <p><b>ë°œê¸‰ì¼ì</b>: {today}</p>
                <p><b>í™˜ì ì§„ìˆ </b>: {user_input}</p>
                <hr>
                <h3 style="color:#1565C0;">ìµœì¢… ì˜ˆì¸¡</h3>
                <p style="font-size:22px; font-weight:bold; color:#D32F2F;">
                    {pred_label} ({pred_conf:.1f}%)
                </p>
                <hr>
                <h3 style="color:#1565C0;">ì „ì²´ ì˜ˆì¸¡ í™•ë¥ </h3>
                <ul>
                    <li>{label_map[0]}: <b>{probs_percent[0]:.1f}%</b></li>
                    <li>{label_map[1]}: <b>{probs_percent[1]:.1f}%</b></li>
                    <li>{label_map[2]}: <b>{probs_percent[2]:.1f}%</b></li>
                </ul>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.success("âœ… ì˜ˆì¸¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¬ê°í•œ ì¦ìƒì´ ì§€ì†ë  ê²½ìš° ì „ë¬¸ì˜ ìƒë‹´ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        # Display explanation
        explanation = generate_llm_explanation(user_input, pred_label)
        st.markdown("### AI ì„¤ëª…")
        st.write(explanation)

# Separator
st.markdown("---")
st.subheader("ğŸ’¬ AI ìƒë‹´ ì±—ë´‡")

# Display previous chat messages
for role, message in st.session_state["chat_history"]:
    with st.chat_message(role):
        st.markdown(message)

# Chat input: if the user types a question, produce an answer
user_query = st.chat_input("ìš°ìš¸ì¦ ê´€ë ¨ ì¼ë°˜ ìƒë‹´, ê°ì • ì¡°ì ˆ, ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
if user_query:
    # Append user message
    st.session_state["chat_history"].append(("user", user_query))
    with st.chat_message("user"):
        st.markdown(user_query)
    # Generate bot reply using last predicted label as context
    reply = chatbot_answer(user_query, st.session_state.get("last_pred_label"))
    st.session_state["chat_history"].append(("assistant", reply))
    with st.chat_message("assistant"):
        st.markdown(reply)


# Footer with additional information
st.markdown("---")
st.markdown("### ì°¸ê³  ì •ë³´")
st.markdown("* Patent title: APPARATUS AND METHOD FOR PREDICTING DEPRESSION LEVELS USING NATURAL LANGUAGE PROCESSING AND EXPLAINABLE ARTIFICIAL INTELLIGENCE")
st.markdown("* Patent number: 10-2024-0119065")
st.markdown("* Developer: Myung-Gwan Kim")
st.markdown("* Applicant: CHA University Industry-Academic Cooperation Foundation")
st.markdown("* Inventors: Myung-Gwan Kim, Hyun Wook Han, DaWoon Wang, JoonHo Park")
