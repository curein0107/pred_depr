import streamlit as st
import joblib
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime

# Try to import the Hugging Face transformers library.  If it's not
# available (for instance due to missing dependencies), we set a
# flag so that the app can fall back to deterministic responses.
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore
    _transformers_available = True
except ImportError:
    _transformers_available = False


# -----------------------------------------------------------------------------
# Model and vectorizer loading
#
# The following section loads the vocabulary and inverse document frequency
# values used to initialize the TFâ€‘IDF vectorizer.  It then loads a
# preâ€‘trained XGBoost model from disk.  These files must reside in the
# same directory as this script.  If they are missing, Streamlit will
# display an appropriate error when the app is executed.
# -----------------------------------------------------------------------------

with open("vocab.json", "r", encoding="utf-8") as f:
    vocab = json.load(f)

# Load IDF values and attach them to a fresh vectorizer
idf = np.load("idf.npy")
tfidf_vectorizer = TfidfVectorizer(vocabulary=vocab)
tfidf_vectorizer.idf_ = idf

# Load the classification model
xgb_model = joblib.load("xgbc_nlp_depression_level_model.pkl")

label_map = {0: "ì •ìƒ", 1: "ê²½ë¯¸í•œ ìš°ìš¸ì¦", 2: "ì¤‘ë“±ë„ ìš°ìš¸ì¦"}


# -----------------------------------------------------------------------------
# Explanation generation
#
# When a prediction is made, this helper function attempts to use a
# lightweight language model to generate a helpful explanation.  If
# transformers are unavailable or an error occurs during generation,
# predefined fallback text is returned based on the predicted label.
# -----------------------------------------------------------------------------

def generate_llm_explanation(
    user_text: str,
    pred_label: str,
    llm_model_name: str = "Qwen/Qwen3-8B-Chat",
    device: str = "cpu",
) -> str:
    """
    Generate a natural language explanation for the predicted label.

    This function attempts to explain the model's prediction by
    summarising the user's statement and inferring potential causes of
    their depressive feelings.  When a language model is available, it
    generates a short empathetic explanation that interprets possible
    stressors or triggers mentioned in the input.  If the model
    cannot be loaded, fallback text is used.

    Parameters
    ----------
    user_text : str
        The original user input text describing their feelings.
    pred_label : str
        The predicted category name (e.g. "ì •ìƒ", "ê²½ë¯¸í•œ ìš°ìš¸ì¦").
    llm_model_name : str, optional
        Name of the Hugging Face model to use for generation.  By default
        this uses ``"Qwen/Qwen3-8B-Chat"``, an 8.2Â billionâ€‘parameter model
        from the Qwen3 series.  This model supports over 100 languages
        including Korean and provides strong reasoning and dialogue
        capabilities at a relatively compact sizeã€793020839043067â€ L149-L181ã€‘.  If
        the environment cannot load this model (e.g. due to limited
        memory), the function will automatically fall back to a smaller
        model such as DistilGPT2.
    device : str, optional
        Device for model execution (e.g. ``"cpu"`` or ``"cuda"``).  Default
        is CPU.

    Returns
    -------
    str
        A Korean explanation describing the prediction in 3â€“5 sentences,
        highlighting possible causes mentioned in the user input.
    """
    # Prompt instructing the language model to infer causes from the input
    prompt = (
        f"ì‚¬ìš©ìì˜ ì§„ìˆ : {user_text}\n"
        f"ëª¨ë¸ì˜ ì˜ˆì¸¡ ìš°ìš¸ì¦ ì¤‘ì¦ë„: {pred_label}\n"
        "ì‚¬ìš©ìê°€ ì„œìˆ í•œ ë‚´ìš©ì—ì„œ ìš°ìš¸ê°ì„ ìœ ë°œí•œ ì£¼ìš” ì›ì¸ì´ë‚˜ ìŠ¤íŠ¸ë ˆìŠ¤ë¡œ ì¶”ì¸¡ë˜ëŠ” ë¶€ë¶„ì„ ì°¾ì•„ "
        "í•´ì„í•´ ì£¼ì„¸ìš”. ê²°ê³¼ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ 3~5ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ê³µê°ì–´ë¦° ì–´ì¡°ë¡œ ê°ì • ê´€ë¦¬ì™€ "
        "ìŠ¤íŠ¸ë ˆìŠ¤ ì™„í™” íŒì„ í¬í•¨í•´ ì£¼ì„¸ìš”. ì˜ë£Œì  ì§„ë‹¨ì€ í•˜ì§€ ë§ˆì„¸ìš”."
    )
    # Use a language model if available
    if _transformers_available:
        try:
            # The GPTâ€‘OSS series uses the Harmony response format.  Using the
            # Transformers chat pipeline with ``messages`` automatically applies
            # the proper formatting.  We set ``torch_dtype" and ``device_map``
            # to ``auto" so that the model will select an appropriate precision
            # and device.  On systems without a GPU or with insufficient
            # memory, this may raise an exception, in which case we fall back
            # to predefined text.
            tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
            model = AutoModelForCausalLM.from_pretrained(
                llm_model_name,
                torch_dtype="auto",
                device_map="auto",
            )
            gen = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                torch_dtype="auto",
                device_map="auto",
            )
            # Construct the conversation with a single user message.  The
            # pipeline will return a list of messages using the Harmony
            # format; the last item contains the assistant's answer.
            messages = [
                {"role": "user", "content": prompt},
            ]
            result = gen(messages, max_new_tokens=160)
            generated_msgs = result[0]["generated_text"]
            # Extract the assistant's reply from the generated messages
            last_msg = generated_msgs[-1]
            # The content field holds the model's response
            if isinstance(last_msg, dict) and "content" in last_msg:
                return last_msg["content"].strip()
            # Fallback: if the structure is unexpected, return the text itself
            return str(last_msg).strip()
        except Exception:
            pass
    # Fallback explanations based solely on the predicted label
    if pred_label == "ì •ìƒ":
        return (
            "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì •ìƒ ë²”ì£¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. ìš°ìš¸ê°ì„ ëŠë¼ì§€ ì•Šë”ë¼ë„ ê·œì¹™ì ì¸ ìš´ë™ê³¼ ì¶©ë¶„í•œ ìˆ˜ë©´, "
            "ê· í˜• ì¡íŒ ì‹ë‹¨, ê°ì‚¬ì¼ê¸° ì“°ê¸° ë“± ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ê¾¸ì¤€íˆ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì •ì‹ ê±´ê°•ì— ë„ì›€ì´ ë©ë‹ˆë‹¤."
        )
    if pred_label == "ê²½ë¯¸í•œ ìš°ìš¸ì¦":
        return (
            "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ëŠë¼ëŠ” ìƒí™©ì„ ì ê²€í•˜ê³ , "
            "ìš´ë™Â·ëª…ìƒÂ·ê°ì‚¬ì¼ê¸° ë“±ìœ¼ë¡œ ë§ˆìŒì„ ë‹¤ìŠ¤ë ¤ ë³´ì„¸ìš”. ê±±ì •ì´ ì§€ì†ë˜ë©´ ì£¼ë³€ì˜ ì§€ì§€ë¥¼ ë°›ê±°ë‚˜ ì „ë¬¸ê°€ ìƒë‹´ì„ í†µí•´ ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    # pred_label == "ì¤‘ë“±ë„ ìš°ìš¸ì¦" or unknown
    return (
        "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ë‚˜ ë‹¤ì–‘í•œ ë¬¸ì œë“¤ì´ ì˜í–¥ì„ ë¯¸ì³¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
        "ì¶©ë¶„í•œ íœ´ì‹ê³¼ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì„ ìœ ì§€í•˜ê³ , ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ì´ì•¼ê¸°í•˜ê±°ë‚˜ ì „ë¬¸ê°€ì—ê²Œ ë„ì›€ì„ ìš”ì²­í•˜ì„¸ìš”. ì‹¬ê°í•œ ì¦ìƒì´ ì§€ì†ë˜ë©´ ì¹˜ë£Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
    )


# -----------------------------------------------------------------------------
# Chatbot helper
#
# This function creates a reply to the user's followâ€‘up questions.  It uses
# a lightweight language model when available and otherwise returns
# preâ€‘defined advice.  The chatbot emphasises that it provides general
# guidance without medical diagnosis.  If a prediction was previously
# generated, the last predicted label can be supplied to contextualise the
# answer.
# -----------------------------------------------------------------------------

def chatbot_answer(user_msg: str, last_pred_label: str | None = None,
                   llm_model_name: str = "Qwen/Qwen3-8B-Chat") -> str:
    """
    Generate a chatbot reply for a user's question.

    Parameters
    ----------
    user_msg : str
        The user's followâ€‘up question or comment.
    last_pred_label : str or None
        The most recent predicted label from the classification model (if any).
    llm_model_name : str, optional
        Name of the language model to use for generation.  The default
        ``"Qwen/Qwen3-8B-Chat"`` is a Koreanâ€‘capable model from the
        Qwen3 series, providing robust multilingual dialogue ability at
        around 8Â billion parametersã€793020839043067â€ L149-L181ã€‘.  If this
        model cannot be loaded, smaller models are used automatically.

    Returns
    -------
    str
        A friendly, empathetic reply in Korean.
    """
    # Base prompt describing the assistant's persona and safety constraints
    # We'll construct a system message describing the chatbot's role
    # and safety constraints.  Including the last prediction label helps
    # personalise the response while maintaining safety.
    system_message = (
        "ë„ˆëŠ” ìš°ìš¸ì¦ ê´€ë ¨ ìƒë‹´ AIì…ë‹ˆë‹¤. ì§„ë‹¨ì„ ë‚´ë¦¬ì§€ ì•Šê³ , ì‚¬ìš©ìê°€ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ "
        "ì¼ë°˜ì ì¸ ì •ë³´ì™€ ê°ì • ì¡°ì ˆ íŒ, ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ ë°©ë²•, ìƒí™œìŠµê´€ ê°œì„  ì¡°ì–¸ ë“±ì„ ì œê³µí•©ë‹ˆë‹¤. "
        "ë˜í•œ ìš°ìš¸ì¦ ì˜ˆì¸¡ ê²°ê³¼ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë•ì§€ë§Œ ì „ë¬¸ì ì¸ ì˜ë£Œ íŒë‹¨ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    )
    if last_pred_label:
        system_message += f" ìµœê·¼ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ëŠ” '{last_pred_label}' ì…ë‹ˆë‹¤."
    system_message += " ì§ˆë¬¸ì— ëŒ€í•´ ê³µê°í•˜ëŠ” ì–´ì¡°ë¡œ 3~5ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."
    # Use transformers if available
    if _transformers_available:
        try:
            tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
            model = AutoModelForCausalLM.from_pretrained(
                llm_model_name,
                torch_dtype="auto",
                device_map="auto",
            )
            gen = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                torch_dtype="auto",
                device_map="auto",
            )
            # Build chat history messages for context
            messages: list[dict[str, str]] = []
            # Append system message at the start
            messages.append({"role": "system", "content": system_message})
            # Append previous chat history
            for role, content in st.session_state.get("chat_history", []):
                # Only include last few messages to avoid hitting token limits
                if role in ("user", "assistant"):
                    messages.append({"role": role, "content": content})
            # Append current user question
            messages.append({"role": "user", "content": user_msg})
            result = gen(messages, max_new_tokens=180)
            generated_msgs = result[0]["generated_text"]
            # The last element contains the assistant's reply
            last_msg = generated_msgs[-1]
            if isinstance(last_msg, dict) and "content" in last_msg:
                return last_msg["content"].strip()
            return str(last_msg).strip()
        except Exception:
            pass
    # Determine severity-specific suggestion templates
    # Each severity has specific advice for different query categories.  If no
    # prediction is available, a general template is used.
    severity_templates = {
        "ì •ìƒ": {
            "stress": "í˜„ì¬ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì •ìƒ ë²”ì£¼ì…ë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ì‚¬, ëª…ìƒê³¼ ê°ì‚¬ì¼ê¸°ë¥¼ í™œìš©í•´ ì¢‹ì€ ìƒíƒœë¥¼ ìœ ì§€í•˜ì„¸ìš”.",
            "emotion": "í˜„ì¬ ì •ìƒ ë²”ì£¼ì´ì§€ë§Œ ê°ì • ê´€ë¦¬ë¥¼ ìœ„í•´ ë§ˆìŒì±™ê¹€ê³¼ í˜¸í¡ë²•, ê°ì • ì¼ê¸°, ê¸ì •ê³¼ ë¶€ì • ê°ì •ì˜ ê· í˜•ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.",
            "lifestyle": "í˜„ì¬ ì •ìƒ ë²”ì£¼ì…ë‹ˆë‹¤. ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ê¾¸ì¤€í•œ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ì¹´í˜ì¸Â·ì•Œì½”ì˜¬ ì œí•œ, ì·¨ë¯¸ í™œë™ì„ ì´ì–´ê°€ì„¸ìš”.",
            "result": "í˜„ì¬ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì •ìƒ ë²”ì£¼ë¡œ íŠ¹ë³„í•œ ìš°ë ¤ëŠ” ì—†ìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ëŠë‚„ ë•Œì—ëŠ” ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ ê¸°ë²•ì„ í™œìš©í•˜ë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤.",
            "default": "í˜„ì¬ ì •ìƒ ë²”ì£¼ì— ì†í•´ ìˆìœ¼ë‹ˆ ì‹ ì²´ì™€ ë§ˆìŒì˜ ê±´ê°•ì„ ì§€í‚¤ê¸° ìœ„í•´ ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ë²•ì„ ê¾¸ì¤€íˆ ì‹¤ì²œí•˜ì„¸ìš”. ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
        },
        "ê²½ë¯¸í•œ ìš°ìš¸ì¦": {
            "stress": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨ê³¼ í•¨ê»˜ ëª…ìƒì´ë‚˜ ê°ì‚¬ì¼ê¸°ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”. ì£¼ë³€ ì‚¬ëŒë“¤ê³¼ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒë„ ë„ì›€ì´ ë©ë‹ˆë‹¤.",
            "emotion": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë‹ˆ ë§ˆìŒì±™ê¹€, í˜¸í¡ë²•, ê°ì • ì¼ê¸° ë“±ì„ í™œìš©í•´ ê°ì •ì„ ê´€ë¦¬í•´ë³´ì„¸ìš”. ê¸ì •ì ì¸ ê²½í—˜ì— ì§‘ì¤‘í•˜ê³ , ê±±ì •ì„ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ì†Œí†µí•˜ì„¸ìš”.",
            "lifestyle": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë¯€ë¡œ ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ë”ìš± ì‹ ê²½ ì¨ì•¼ í•©ë‹ˆë‹¤. ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ì¹´í˜ì¸Â·ì•Œì½”ì˜¬ ì ˆì œ, í¡ì—°ê³¼ ì•½ë¬¼ í”¼í•˜ê¸° ë“±ì´ ë„ì›€ì´ ë©ë‹ˆë‹¤. ë˜í•œ ì¦ê±°ì›€ì„ ëŠë‚„ ìˆ˜ ìˆëŠ” ì·¨ë¯¸ì™€ í™œë™ì„ ì§€ì†í•˜ì„¸ìš”.",
            "result": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆì§€ë§Œ ì ì ˆí•œ ê´€ë¦¬ë¡œ ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê±±ì •ì´ ì§€ì†ë˜ë©´ ìƒë‹´ì„ ê¶Œìœ ë°›ìœ¼ì‹œê³ , ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ì™€ ìƒí™œìŠµê´€ ê°œì„ ì„ í†µí•´ ì™„í™”ë¥¼ ë„ëª¨í•˜ì„¸ìš”.",
            "default": "ê²½ë¯¸í•œ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë‹ˆ ê¸°ë¶„ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ë²•ì„ ê¾¸ì¤€íˆ ì‹¤ì²œí•˜ê³ , í•„ìš”í•˜ë©´ ì£¼ë³€ ì‚¬ëŒë“¤ê³¼ ì´ì•¼ê¸°í•˜ê±°ë‚˜ ìƒë‹´ì„ ê³ ë ¤í•´ë³´ì„¸ìš”. ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”."
        },
        "ì¤‘ë“±ë„ ìš°ìš¸ì¦": {
            "stress": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ê°€ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤. ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ëª…ìƒê³¼ ê°ì‚¬ì¼ê¸°ë¥¼ ì‹¤ì²œí•˜ì„¸ìš”. ë˜í•œ, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ë§ˆìŒì„ ë‚˜ëˆ„ê³  í•„ìš”í•˜ë‹¤ë©´ ì „ë¬¸ì ì¸ ìƒë‹´ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.",
            "emotion": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë¯€ë¡œ ê°ì • ê´€ë¦¬ì— ë”ìš± ì‹ ê²½ì„ ì¨ì•¼ í•©ë‹ˆë‹¤. ë§ˆìŒì±™ê¹€ê³¼ í˜¸í¡ë²•ì„ í†µí•´ ê°ì •ì„ ì •ë¦¬í•˜ê³ , ê°ì •ì¼ê¸°ë¥¼ ì¨ë³´ì„¸ìš”. ì£¼ë³€ì˜ ì§€ì§€ë§ì„ í™œìš©í•˜ê³  ë¶€ì •ì ì¸ ìƒê°ì„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ê³µìœ í•˜ì„¸ìš”.",
            "lifestyle": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë¯€ë¡œ ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ë”ë¶ˆì–´ ì „ë¬¸ì ì¸ ì§€ì›ì„ ë°›ì„ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ê¾¸ì¤€í•œ ìš´ë™ê³¼ ê· í˜• ì¡íŒ ì‹ë‹¨, ì¶©ë¶„í•œ ìˆ˜ë©´, ì¹´í˜ì¸Â·ì•Œì½”ì˜¬ ì œí•œ, í¡ì—°Â·ì•½ë¬¼ í”¼í•˜ê¸°ë¥¼ ì‹¤ì²œí•˜ì„¸ìš”. ì‹¬ê°í•œ ì¦ìƒì´ ì§€ì†ë  ê²½ìš° ì •ì‹ ê±´ê°• ì „ë¬¸ê°€ì—ê²Œ ìƒë‹´ì„ ë°›ì•„ë³´ì„¸ìš”.",
            "result": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬, ì „ë¬¸ì ì¸ ë„ì›€ì„ ë°›ê³  ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ë²•ì„ ì‹¤ì²œí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì£¼ë³€ì¸ê³¼ì˜ ì†Œí†µê³¼ ìƒë‹´ì„ í†µí•´ ì§€ì§€ë¥¼ ë°›ìœ¼ì„¸ìš”.",
            "default": "ì¤‘ë“±ë„ ìš°ìš¸ ì¦ìƒì´ ìˆìœ¼ë¯€ë¡œ ìŠ¤ìŠ¤ë¡œë¥¼ ëŒë³´ê³  ì „ë¬¸ì ì¸ ì§€ì›ì„ ë°›ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ì™€ ìƒí™œìŠµê´€ ê°œì„ ì„ ê¾¸ì¤€íˆ ì‹¤ì²œí•˜ê³ , í•„ìš”í•˜ë©´ ì „ë¬¸ê¸°ê´€ì´ë‚˜ ì „ë¬¸ê°€ì˜ ë„ì›€ì„ ë°›ì•„ë³´ì„¸ìš”. ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”."
        }
    }
    # Normalise message for keyword detection
    msg = user_msg.lower()
    # Determine which template to use based on last prediction
    template = severity_templates.get(last_pred_label) if last_pred_label in severity_templates else None
    # Define category keys
    category = None
    if any(keyword in msg for keyword in ["ìŠ¤íŠ¸ë ˆìŠ¤", "stress"]):
        category = "stress"
    elif any(keyword in msg for keyword in ["ê°ì •", "ì¡°ì ˆ", "emotion"]):
        category = "emotion"
    elif any(keyword in msg for keyword in ["ìƒí™œ", "ìŠµê´€", "lifestyle"]):
        category = "lifestyle"
    elif any(keyword in msg for keyword in ["ê²°ê³¼", "ì˜ˆì¸¡", "í•´ì„"]):
        category = "result"
    # If we have a template for this severity, return the corresponding message
    if template:
        if category and category in template:
            return template[category]
        # default category message
        return template["default"]
    # Generic responses when no prediction is available
    generic = {
        "stress": "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ì‹¬í˜¸í¡ê³¼ ëª…ìƒ, ê°ì‚¬ì¼ê¸°, ìì—° ì† ì‚°ì±…, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ì˜ ëŒ€í™”ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.",
        "emotion": "ê°ì •ì„ ì¡°ì ˆí•˜ê¸° ìœ„í•´ ë§ˆìŒì±™ê¹€ê³¼ í˜¸í¡ë²•ì„ í†µí•´ í˜„ì¬ ìˆœê°„ì— ì§‘ì¤‘í•˜ê³ , ê°ì •ì¼ê¸°ë¥¼ ì¨ì„œ ê°ì •ì„ í‘œí˜„í•´ë³´ì„¸ìš”. ê¸ì •ê³¼ ë¶€ì • ê°ì •ì˜ ê· í˜•ì„ ìœ ì§€í•˜ê³  ê°ì‚¬í•˜ëŠ” ë§ˆìŒì„ ê°–ëŠ” ê²ƒì´ ë„ì›€ì´ ë©ë‹ˆë‹¤.",
        "lifestyle": "ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ìœ„í•´ ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ê· í˜• ì¡íŒ ì‹ë‹¨, ì¹´í˜ì¸Â·ì•Œì½”ì˜¬ ì œí•œ, í¡ì—°Â·ì•½ë¬¼ í”¼í•˜ê¸°ë¥¼ ì‹¤ì²œí•˜ì„¸ìš”. ê°€ì¡±ê³¼ ì¹œêµ¬ë“¤ê³¼ ì‹œê°„ì„ ë³´ë‚´ê³  ì·¨ë¯¸ í™œë™ì„ ì´ì–´ê°€ëŠ” ê²ƒë„ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "result": "ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤. ê²°ê³¼ê°€ ê±±ì •ë˜ë©´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒë“¤ê³¼ ìƒë‹´í•˜ê±°ë‚˜ ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì„ êµ¬í•˜ì„¸ìš”. ê±´ê°•í•œ ìƒí™œìŠµê´€ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ê°€ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "default": "ì§ˆë¬¸ì„ í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì €ëŠ” ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ì œê³µí•˜ëŠ” ìƒë‹´ AIì´ë©°, ì§„ë‹¨ì„ ë‚´ë¦¬ê±°ë‚˜ ì¹˜ë£Œë¥¼ ëŒ€ì‹ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒí™œìŠµê´€ ê°œì„ ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬, ê¸ì •ì ì¸ ë§ˆìŒê°€ì§ì„ í†µí•´ ì •ì‹ ê±´ê°•ì„ ëŒë³´ì„¸ìš”. ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
    }
    if category and category in generic:
        return generic[category]
    return generic["default"]


# -----------------------------------------------------------------------------
# Streamlit application layout
#
# The following section defines the user interface elements: text input for
# describing current feelings, a prediction trigger button, display of
# probabilities and interpretive information, and a chat interface for
# followâ€‘up questions.
# -----------------------------------------------------------------------------

st.set_page_config(page_title="ìš°ìš¸ì¦ ì˜ˆì¸¡ ë° ìƒë‹´", layout="centered")
st.title("ğŸ§  ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ìš°ìš¸ì¦ ì˜ˆì¸¡ ë° ìƒë‹´")

st.markdown(
    "ë³¸ ì†Œí”„íŠ¸ì›¨ì–´ëŠ” ë¶„ë‹¹ì°¨ë³‘ì› ì •ì‹ ê±´ê°•ì˜í•™ê³¼ ì „ë¬¸ì˜ì˜ ìš°ìš¸ì¦ ì§„ë‹¨ ê²½í—˜ì„ "
    "ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ í•™ìŠµí•˜ì—¬, ë¬¸ì¥ë§Œìœ¼ë¡œ ìš°ìš¸ì¦ ì¤‘ì¦ë„ë¥¼ ì˜ˆì¸¡í•˜ê³  ê´€ë ¨ ìƒë‹´ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. "
    "**ì´ ì•±ì˜ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©° ì •ì‹ ê±´ê°•ì˜í•™ê³¼ ì „ë¬¸ì˜ì˜ ì§„ë‹¨ì„ ëŒ€ì²´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**"
)

# User input area
user_input = st.text_area("í˜„ì¬ ëŠë¼ê³  ìˆëŠ” ê°ì •ì„ ë¬¸ì¥ìœ¼ë¡œ ì…ë ¥í•´ë³´ì„¸ìš”.", height=150)

# Initialize session state for predictions and chat
if "last_pred_label" not in st.session_state:
    st.session_state["last_pred_label"] = None
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []  # list of (role, message)


if st.button("ì§„ë‹¨í•˜ê¸°"):
    if not user_input.strip():
        st.warning("âš ï¸ ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # Vectorize and predict
        vec = tfidf_vectorizer.transform([user_input])
        probs = xgb_model.predict_proba(vec)[0]
        probs_percent = np.round(probs * 100, 1)
        pred_idx = int(np.argmax(probs_percent))
        pred_label = label_map[pred_idx]
        pred_conf = probs_percent[pred_idx]
        st.session_state["last_pred_label"] = pred_label
        today = datetime.today().strftime("%Y-%m-%d")
        # Display results card
        st.markdown(
            f"""
            <div style="border:2px solid #4CAF50; border-radius:10px; padding:20px; background-color:#f9fff9;">
                <h2 style="color:#2E7D32; text-align:center;">ğŸ§¾ ì¸ê³µì§€ëŠ¥ ì˜ˆì¸¡ ê²°ê³¼</h2>
                <p><b>ë°œê¸‰ì¼ì</b>: {today}</p>
                <p><b>í™˜ì ì§„ìˆ </b>: {user_input}</p>
                <hr>
                <h3 style="color:#1565C0;">ìµœì¢… ì˜ˆì¸¡</h3>
                <p style="font-size:22px; font-weight:bold; color:#D32F2F;">
                    {pred_label} ({pred_conf:.1f}%)
                </p>
                <hr>
                <h3 style="color:#1565C0;">ì „ì²´ ì˜ˆì¸¡ í™•ë¥ </h3>
                <ul>
                    <li>{label_map[0]}: <b>{probs_percent[0]:.1f}%</b></li>
                    <li>{label_map[1]}: <b>{probs_percent[1]:.1f}%</b></li>
                    <li>{label_map[2]}: <b>{probs_percent[2]:.1f}%</b></li>
                </ul>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.success("âœ… ì˜ˆì¸¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¬ê°í•œ ì¦ìƒì´ ì§€ì†ë  ê²½ìš° ì „ë¬¸ì˜ ìƒë‹´ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        # Display explanation
        explanation = generate_llm_explanation(user_input, pred_label)
        st.markdown("### AI ì„¤ëª…")
        st.write(explanation)

# Separator
st.markdown("---")
st.subheader("ğŸ’¬ AI ìƒë‹´ ì±—ë´‡")

# Display previous chat messages
for role, message in st.session_state["chat_history"]:
    with st.chat_message(role):
        st.markdown(message)

# Chat input using Streamlit's chat widget.  ``st.chat_input`` appears
# below previous messages and allows users to press Enter to send.  This
# provides a more natural chat experience compared to separate text
# fields and buttons.
chat_prompt = st.chat_input(
    "ì±—ë´‡ì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš” (ìš°ìš¸ì¦ ê´€ë ¨ ìƒë‹´, ê°ì • ì¡°ì ˆ, ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬, ìƒí™œìŠµê´€ ê°œì„  ë“±)",
    key="chat_input",
)

# When the user submits a question via the chat input
if chat_prompt:
    # Record the user question
    st.session_state["chat_history"].append(("user", chat_prompt))
    with st.chat_message("user"):
        st.markdown(chat_prompt)
    # Generate reply using last predicted label if available
    reply = chatbot_answer(chat_prompt, st.session_state.get("last_pred_label"))
    st.session_state["chat_history"].append(("assistant", reply))
    with st.chat_message("assistant"):
        st.markdown(reply)


# Footer with additional information
st.markdown("---")
st.markdown("### ì°¸ê³  ì •ë³´")
st.markdown("* Patent title: APPARATUS AND METHOD FOR PREDICTING DEPRESSION LEVELS USING NATURAL LANGUAGE PROCESSING AND EXPLAINABLE ARTIFICIAL INTELLIGENCE")
st.markdown("* Patent number: 10-2024-0119065")
st.markdown("* Developer: Myung-Gwan Kim")
st.markdown("* Applicant: CHA University Industry-Academic Cooperation Foundation")
st.markdown("* Inventors: Myung-Gwan Kim, Hyun Wook Han, DaWoon Wang, JoonHo Park")
